# Cygnus Internship AI Task

## **ðŸ§  AI Challenge â€“ Choose Your Path**

Youâ€™re invited to pick **one of two hands-on GenAI tasks**â€”both designed to test your ability to work with **local LLMs using Ollama**. Each task focuses on building, deploying, and interacting with powerful open-source models like **Mistral**, **LLaMA3**, or **Gemma**, completely offline.

Choose **either Task 1 or Task 2**, depending on your interests and strengths. Whichever you pick, the goal is to build something working end-to-end using real data and local inference.

---

### **ðŸš€ Task 1: Build a Local AI Q&A Bot using Ollama + LangChain**

**Objective:**

Create a local **RAG (Retrieval-Augmented Generation)** chatbot that answers user questions based on uploaded documents (PDFs or text). It should work entirely offline using Ollama and a local vector store.

---

### **Step-by-Step Breakdown:**

1. **Model Setup:**
    - Install Ollama and download a model like mistral, llama3, or gemma.
    - Ensure it runs locally and is available for inference.
2. **Document Ingestion:**
    - Load documents (PDF, .txt, .md, etc.).
    - Split the content into manageable chunks (use recursive text splitting or similar).
    - Generate embeddings using:
        - ollama/embedding-mistral, or
        - sentence-transformers (like all-MiniLM).
    - Store these vectors in **ChromaDB**, **FAISS**, or another lightweight vector DB.
3. **RAG Setup with LangChain or LlamaIndex:**
    - On receiving a user query, retrieve relevant chunks using cosine similarity.
    - Use Ollama to generate a response based on the retrieved chunks (i.e., context + query).
    - Display the final answer in a clean, readable format.
4. **Build a Chat Interface:**
    - Use **Gradio** or **Streamlit** to create a simple web UI.
    - Let users:
        - Ask questions
        - See cited document context
        - Upload new files
    - CLI is also acceptable if preferred.
5. **Bonus Features (Optional):**
    - Add support for **chat history or memory** (LangChain ConversationBufferMemory).
    - Enable switching between different local models to compare outputs.
    - Allow users to upload files dynamically for live ingestion.
6. **Testing:**
    - Ask a variety of questions (factual, follow-up, specific, etc.).
    - Check for hallucinations and latency.
    - Capture failure cases and include notes on improvements.
7. **Deliverables:**
    - Complete codebase with requirements.txt or environment.yml
    - Gradio/Streamlit interface or CLI demo
    - Sample document set + 5â€“10 queries
    - README file with setup steps and usage instructions

---

### **ðŸ”§ Task 2: Fine-Tune and Serve a Domain-Specific LLM with Ollama**

**Objective:**

Fine-tune an open-weight LLM like **Mistral 7B** or **LLaMA3** on a **specific domain** (like education, customer support, or travel) and serve it locally via **Ollama**. Compare it with the base model in terms of quality and performance.

---

### **Step-by-Step Breakdown:**

1. **Data Curation:**
    - Create or gather 1,000â€“10,000 high-quality instructionâ€“response pairs.
    - Format (Alpaca-style):

```
{
  "instruction": "Whatâ€™s the policy for rescheduling a flight?",
  "response": "To reschedule, visit 'My Bookings' and click on 'Reschedule'."
}
```

1. 
    - 
    - Choose a relevant domain: Education, Fintech, HR, Travel, etc.
2. **Base Model Setup:**
    - Choose and download your base model (mistral, llama2, or llama3) via HuggingFace.
    - Quantize if needed (GGUF format) and prepare to load using Ollama.
3. **Fine-Tuning:**
    - Use **LoRA** or **QLoRA** with frameworks like:
        - HuggingFace Transformers + PEFT
        - Axolotl, Unsloth, or AutoTrain
    - Fine-tune on GPU (Colab, RunPod, or local if available).
    - Convert final model weights to GGUF format.
4. **Serving the Model:**
    - Bundle the model using an Ollama Modelfile:

```
FROM llama3
PARAMETER temperature 0.7
```

1. 
    - 
    - Test that the fine-tuned model runs smoothly via ollama run.
2. **Interface + Evaluation:**
    - Build a CLI or web interface to:
        - Query both **base** and **fine-tuned** models.
        - Show side-by-side outputs.
    - Benchmark using:
        - Predefined prompts
        - Domain-specific questions
        - Latency and hallucination tracking
3. **Bonus Tasks:**
    - Combine your **fine-tuned model with a RAG setup**.
    - Use **LangChain tools** over your custom Ollama model.
    - Package the solution with setup scripts to run on another machine.
4. **Deliverables:**
    - Fine-tuned model + training script
    - Sample dataset + evaluation prompts
    - Web/CLI interface
    - GitHub repo with README and usage instructions

---

### **ðŸ“Š Evaluation Criteria (for both tasks):**

| **Criteria** | **Weight** |
| --- | --- |
| Understanding of pipeline and architecture | 25% |
| Quality of data processing and model use | 20% |
| Local integration with Ollama | 20% |
| Testing & comparison (accuracy, latency) | 15% |
| Interface, UX, documentation | 10% |
| Bonus (RAG + fine-tuned, tools, multi-model) | 10% |

---

### **ðŸ“® Contact**

Have questions or blockers? Reach out to **niyas@bookcygnus.com** for clarifications.

---

Pick a task based on your expertise - whether you want to build a smart offline Q&A bot or train a domain expert LLM that understands your users. Either way, youâ€™ll come out with a production-ready local AI application.

**Let the building begin ðŸš€**

https://zrec.in/EYZLa